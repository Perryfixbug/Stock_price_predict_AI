from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import streamlit as st

# MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.1"  # Nh·∫π v√† ch·∫°y ·ªïn ƒë·ªãnh tr√™n CPU

# model = AutoModelForCausalLM.from_pretrained(
#     MODEL_NAME,
#     torch_dtype=torch.float16,
#     device_map="auto",
#     token=True
# )

# tokenizer = AutoTokenizer.from_pretrained(
#     MODEL_NAME,
#     token=True
# )

# # ƒê·∫£m b·∫£o c√≥ pad_token
# if tokenizer.pad_token is None:
#     tokenizer.pad_token = tokenizer.eos_token

@st.cache_resource(show_spinner="ü§ñ ƒêang t·∫£i m√¥ h√¨nh AI t√†i ch√≠nh...")
def load_model():
    model = AutoModelForCausalLM.from_pretrained(
        "mistralai/Mistral-7B-Instruct-v0.1",
        torch_dtype=torch.float16,
        device_map="auto",
        use_auth_token=True  # ho·∫∑c d√πng token="your_token"
    )
    tokenizer = AutoTokenizer.from_pretrained(
        "mistralai/Mistral-7B-Instruct-v0.1",
        use_auth_token=True
    )
    tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token
    return model, tokenizer

model, tokenizer = load_model()  # ch·ªâ g·ªçi m·ªôt l·∫ßn khi l·∫ßn ƒë·∫ßu load app

def generate_reply(prompt: str) -> str:
    """
    Chatbot t√†i ch√≠nh d√πng Mistral-7B-Instruct
    """

    # M√¥ t·∫£ vai tr√≤ v√† y√™u c·∫ßu tr·∫£ l·ªùi
    instruction = (
        "You are a helpful and knowledgeable financial assistant. "
        "Answer the question clearly and concisely."
    )

    # ƒê·ªãnh d·∫°ng prompt chu·∫©n cho Mistral-7B-Instruct
    full_prompt = f"<s>[INST] {instruction}\n\n{prompt} [/INST]"

    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=70,
        do_sample=True,
        top_p=0.9,
        temperature=0.7,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        # early_stopping=True
    )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # T√°ch ph·∫ßn sau [/INST]
    if '[/INST]' in decoded:
        answer = decoded.split('[/INST]')[-1].strip()
    else:
        answer = decoded.strip()

    return answer

